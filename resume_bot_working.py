# Reference - Chat GPT

# -*- coding: utf-8 -*-
"""Resume_bot_Working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13gpMo3EoisUiqDC50NgceAI3icyscUKx
"""

pip install qdrant-client[fastembed] dspy-ai dspy-qdrant python-dotenv

# -*- coding: utf-8 -*-
"""
Resume Q&A Bot with DSPy + Qdrant
Updated version with fixed resume path and optimized chunking
"""
import os
from dotenv import load_dotenv
from qdrant_client import QdrantClient, models
from qdrant_client.models import Filter
from dspy_qdrant import QdrantRM
import dspy
import re
from typing import List, Dict, Any

# Load environment variables
load_dotenv()

# Set these in your environment or directly here:
os.environ["QDRANT_CLOUD_URL"] = "https://37636242-68f3-42eb-bf03-070af2870e21.us-east4-0.gcp.cloud.qdrant.io"
os.environ["QDRANT_API_KEY"] = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.BR4DSofdNLhI645sQO4a_X-F6vAVkO_Quj_1V5ae63w"
os.environ["OPENAI_API_KEY"] = "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"
os.environ["OPENAI_MODEL"] = "gpt-4o-mini"
# -------------------------
# Configuration
# -------------------------
COLLECTION_NAME = os.environ.get("QDRANT_COLLECTION_NAME", "resume_chat_bot")
# Update this to the full path of your resume or set RESUME_PATH in .env

RESUME_PATH = os.environ.get("RESUME_PATH", r"C:\Users\npall\Desktop\sample_resume.txt")
#RESUME_PATH = os.environ.get("RESUME_PATH", "sample_resume.txt")

QDRANT_URL = os.environ.get("QDRANT_CLOUD_URL")
QDRANT_API_KEY = os.environ.get("QDRANT_API_KEY")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# Create a dummy fallback_resume.txt file to prevent FileNotFoundError
dummy_content = """

PALLAVI NAGOTHI
San Jose, CA |   +15134603211| □ •  npallavi0905@gmail.com | LinkedIn | Tableau Public

Strategic and solutions-driven Analytics Engineer with 8+ years of experience transforming legacy reporting into intelligent, automated dashboards using Tableau, Power BI, SSRS, and Cognos. Adept at aligning business goals with data strategy, translating complex requirements into compelling visual narratives, and streamlining reporting workflows across large-scale, cross-functional teams. Proven success in healthcare, finance, insurance, and aviation domains—delivering KPI-rich dashboards, accelerating decision-making, and ensuring compliance with HIPAA and industry standards. Skilled in SQL, Python, and Alteryx with a passion for mentoring teams and driving data integrity, visualization standardization, and reporting excellence.

Tableau | Power BI Reporting | Cognos BERT Reports | SSRS Report Development | Alteryx | Data Visualization | AWS | Data Pipelining | SQL Query Optimization | Python Automation | KPI Analysis | Agile/Scrum Methodologies | Data Validation & QA | Stakeholder Communication | Dashboard Standardization and performance improvemnent


Role: Recruited to automate legacy Excel-based reporting, transitioning to dynamic Tableau, SSRS, and BERT Cognos reports while standardizing dashboards, mentoring teams, validating KPIs, and managing ServiceNow work tickets.
•	Replaced legacy Excel reports with dynamic dashboards, reducing manual processes by 35% and improving reporting accuracy and consistency, ensuring flawless, time-critical month-end, quarter-end data delivery.
•	Developed and automated dynamic dashboards using Tableau, Power BI, SSRS, and BERT Cognos, replacing legacy Excel reports to enhance data accuracy and visualization and service ticket workflows for Boeing.
•	Standardized reporting frameworks and validated KPIs, ensuring consistency, reliability, and alignment with business goals across functions.
•	Mentored junior analysts and offshore teams, enabling faster ramp-up on tools, processes, and data governance best practices.
•	Managed and resolved ServiceNow work tickets and ServiceNow's IT Operations Management software by ensuring data quality and addressing report-related issues and implementing enhancements to support evolving user requirements.

Role: Selected as Business Data Analyst to bridge business goals with data strategy by translating complex requirements into actionable insights, enhancing decision-making through analytical storytelling, and streamlining reporting processes across cross- functional teams.
•	Revamped legacy Medicaid systems by crafting insightful Tableau and Power BI dashboards, accelerating claims analysis and ensuring seamless HIPAA-compliant data transitions with exceptional precision and clarity.
•	Elevated reporting accuracy by enhancing complex Tableau and Power BI dashboards, standardizing visuals aligned with evolving client metrics.
•	Conducted advanced data analysis of large healthcare datasets, uncovering trends and inefficiencies that contributed to operational improvements and cost reductions.
•	Managed data pipelines and optimized ETL processes, reducing data processing times by 25%, and enhancing the timeliness of decision-making.


Role: Recruited to streamline data storytelling and user-centric insights by developing real-time Tableau dashboards, optimizing SQL performance, and aligning BI solutions with evolving business needs and risk gaps for key clients.
•	Revamped a fragmented Information System for Amex into a cohesive, user-adaptive platform—boosting client care quality and operational excellence through compelling Tableau visualizations.
•	Crafted high-impact Tableau dashboards showcasing cost trends of nationwide bank projects for Bank of America, enabling senior leadership to make swift, data-backed investment decisions.
•	Seamlessly migrated legacy Cognos dashboards to vibrant Tableau visuals, empowering executives with accurate travel forecasts and agile KPI-driven decision-making across global markets for AMEX.
•	Facilitated business process mapping and identified inefficiencies in data workflows, Led the optimization of reporting processes by implementing data automation and reducing cycle time by 30%.

Role: Hired to transform academic and payroll insights into strategic assets by designing dynamic Cognos dashboards, resolving performance bottlenecks, and enabling mobile-friendly, offline reporting for data-driven institutional decisions for client-Rakesh Institution.
•	Reduced report generation time by 40% through optimization of complex Cognos queries and implementation of concurrent query execution.
•	Enabled 100+ faculty and administrators to access real-time student performance data via mobile-optimized active reports, enhancing academic decision-making efficiency.

•	Masters in Software Engineering | Portland State University | 2016
•	Bachelors in Information Technology | Jawaharlal Nehru Technological University | 2009

•	BI Tools: Power BI 1.14, Tableau 2022, 2019.2/10.5/10.4/9.02, SSRS (2012), Cognos BI suite (10.X), Alteryx
•	Database Languages: T-SQL, PL/SQL, MySQL
•	Databases: Microsoft SQL Server 2016/2014/2012, NOSQL, Oracle 10 g/9i, DB2, MySQL, Amazon Simple DB, Mongo DB, Snowflake
•	Cloud Platforms & Data Solutions: AWS (Amazon Redshift), Azure Data Factory, Cloud Data Integration
•	Database Technologies & Data Integration: SQL Server, Oracle, MySQL, Snowflake, MongoDB, Amazon Redshift, ETL Processes
•	Automation & Data Engineering: Python (Data Analysis, Workflow Automation), SQL/PL-SQL (Complex Queries, Performance Tuning, Data Transformation)
•	Data Visualization & BI Tools: Tableau (Advanced), Power BI, SSRS, Cognos, QlikView, Interactive Dashboards, Real- time Analytics, Data Storytelling
•	Business Analysis & Requirements Gathering: Stakeholder Engagement, Business Process Reengineering, Requirements Definition, Gap Analysis, User Story Mapping, Solution Design, Business Case Development.
•	Advanced Data Analytics & Reporting: Predictive Analytics, Statistical Modelling, Data Mining, Root Cause Analysis, Trend Analysis, Financial & Operational Reporting
•	ETL Tool: SSIS, Informatica
•	Languages: Java, Python, HTML, XML, Android App development, UML
•	IDE’s/Tools: Visual Studio, SSDT, MS Excel, Excel Reports, Visio, SharePoint, MS Project, MS Word, Service Now ITSM
•	Software Methodologies: Waterfall, Agile – Scrum, RUP
•	Project Management & Agile Methodology: Agile, Scrum, Jira, Azure DevOps, MS Project, Cross-Functional Team Collaboration.


"""

with open("fallback_resume.txt", "w") as f:
    f.write(dummy_content)

print("Created dummy fallback_resume.txt")

# -------------------------
# Helpers: text loading + chunking
# -------------------------

def load_resume_text(path: str) -> str:
    # Try to load the main file path
    if not os.path.exists(path):
        print(f"Warning: Resume text file not found at '{path}'. Using fallback path.")
        # Specify an alternative fallback path here, making sure it's valid
        fallback_path = "fallback_resume.txt"  # Make sure this file exists or specify another correct path
        if not os.path.exists(fallback_path):
            raise FileNotFoundError(f"Resume text file not found at '{fallback_path}'. Please check the path.")
        path = fallback_path  # Update the path to fallback path

    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def simple_chunk(text: str, max_tokens: int = 220, overlap: int = 40) -> List[str]:
    """Very simple word-based chunker approximating tokens with words."""
    words = text.split()
    chunks = []
    step = max(1, max_tokens - overlap)
    for start in range(0, len(words), step):
        end = min(len(words), start + max_tokens)
        chunk = " ".join(words[start:end]).strip()
        if chunk:
            chunks.append(chunk)
        if end == len(words):
            break
    return chunks

# -------------------------
# Qdrant setup + indexing
# -------------------------
def connect_qdrant() -> QdrantClient:
    if not QDRANT_URL or not QDRANT_API_KEY:
        raise RuntimeError("Missing Qdrant credentials. Set QDRANT_CLOUD_URL and QDRANT_API_KEY.")
    client = QdrantClient(
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        timeout=60.0,
        prefer_grpc=True,
    )
    return client

def ensure_collection(client: QdrantClient):
    """Create collection with dense + colbert multivectors."""
    exists = client.collection_exists(COLLECTION_NAME)
    if not exists:
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config={
                "dense": models.VectorParams(size=384, distance=models.Distance.COSINE),
                "colbert": models.VectorParams(
                    size=128,
                    distance=models.Distance.COSINE,
                    multivector_config=models.MultiVectorConfig(
                        comparator=models.MultiVectorComparator.MAX_SIM
                    ),
                    hnsw_config=models.HnswConfigDiff(m=0)
                )
            }
        )

def embed_and_upsert_resume(client: QdrantClient, chunks: List[str]):
    """Encode chunks with FastEmbed dense + ColBERT and upsert as multivector points."""
    from fastembed import TextEmbedding, LateInteractionTextEmbedding

    dense_model = TextEmbedding("BAAI/bge-small-en")
    colbert_model = LateInteractionTextEmbedding("colbert-ir/colbertv2.0")

    points = []
    for i, chunk in enumerate(chunks):
        dense_doc = models.Document(text=chunk, model="BAAI/bge-small-en")
        colbert_doc = models.Document(text=chunk, model="colbert-ir/colbertv2.0")

        points.append(
            models.PointStruct(
                id=i,
                vector={
                    "dense": dense_doc,
                    "colbert": colbert_doc
                },
                payload={
                    "chunk_text": chunk,
                    "category": "resume",
                    "source": "user_resume"
                }
            )
        )

    # Batch to avoid payload limits
    BATCH = 8
    for start in range(0, len(points), BATCH):
        batch = points[start:start + BATCH]
        client.upsert(collection_name=COLLECTION_NAME, points=batch)

# -------------------------
# DSPy Integration
# -------------------------
def init_lm() -> dspy.LM:
    """Initialize the LM for DSPy."""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("Set OPENAI_API_KEY for language model access.")
    lm = dspy.LM(OPENAI_MODEL, max_tokens=512, api_key=api_key)
    return lm

def init_rm(client: QdrantClient) -> QdrantRM:
    rm = QdrantRM(
        qdrant_collection_name=COLLECTION_NAME,
        qdrant_client=client,
        vector_name="dense",
        document_field="chunk_text",
        k=8
    )
    return rm

# -------------------------
# ColBERT reranker using native multivector query
# -------------------------
def rerank_with_colbert(client: QdrantClient, query_text: str, limit: int = 5) -> List[str]:
    from fastembed import TextEmbedding, LateInteractionTextEmbedding

    dense_model = TextEmbedding("BAAI/bge-small-en")
    colbert_model = LateInteractionTextEmbedding("colbert-ir/colbertv2.0")

    dense_query = list(dense_model.embed(query_text))[0]
    colbert_query = list(colbert_model.embed(query_text))[0]

    results = client.query_points(
        collection_name=COLLECTION_NAME,
        prefetch=models.Prefetch(query=dense_query, using="dense"),
        query=colbert_query,
        using="colbert",
        limit=limit,
        with_payload=True,
        query_filter=Filter(must=[]),
    )

    docs = []
    for p in results.points:
        docs.append(p.payload.get("chunk_text", ""))
    return docs


# -------------------------
# Guardrails: classify inputs
# -------------------------
ABUSE_LIST = {"idiot", "stupid", "dumb", "shut up", "moron", "trash", "bastard", "jerk"}

def is_abusive(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in ABUSE_LIST)

SMALL_TALK_PATTERNS = [
    r"^\s*hi\s*$", r"^\s*hello\s*$", r"^\s*hey\s*$",
    r"how are you\??", r"what'?s up\??", r"good (morning|afternoon|evening)"
]

def is_small_talk(text: str) -> bool:
    t = text.strip().lower()
    for pat in SMALL_TALK_PATTERNS:
        if re.search(pat, t):
            return True
    return False

# -------------------------
# DSPy Signature & Module
# -------------------------
class ResumeAnswer(dspy.Signature):
    question = dspy.InputField(desc="User question about the resume")
    context = dspy.InputField(desc="Relevant resume text chunks")
    final_answer = dspy.OutputField(desc="Helpful answer grounded in the resume, or a polite response")

class ResumeRAG(dspy.Module):
    def __init__(self, client: QdrantClient):
        super().__init__()
        self.client = client

    def forward(self, question: str):
        # 1) Guardrails
        if is_abusive(question):
            class Dummy:
                final_answer = "I won’t engage with abusive language. If you have a genuine question about my resume, I’m happy to help."
            return Dummy()

        if is_small_talk(question):
            class Dummy:
                final_answer = "Hi! I’m your resume bot. Ask me about experience, projects, strengths, tools, or anything on the resume."
            return Dummy()

        # 2) Retrieve + rerank
        reranked = rerank_with_colbert(self.client, question, limit=6)
        context = "\n".join(reranked)

        # 3) Compose a grounded answer with DSPy
        prompt = dspy.ChainOfThought(ResumeAnswer)
        return prompt(question=question, context=context)

# -------------------------
# Main (index + test)
# -------------------------

def main():
    # 1) Load + chunk resume
    resume_text = load_resume_text(RESUME_PATH)
    chunks = simple_chunk(resume_text, max_tokens=220, overlap=40)
    print(f"Loaded resume: {len(resume_text)} chars, split into {len(chunks)} chunks.")

    # 2) Connect + ensure collection + upsert (idempotent upserts are fine for demo)
    client = connect_qdrant()
    ensure_collection(client)
    embed_and_upsert_resume(client, chunks)
    print(f"Indexed {len(chunks)} chunks into Qdrant collection '{COLLECTION_NAME}'.")

    # 3) Init LM + RM for DSPy
    lm = init_lm()
    rm = init_rm(client)
    dspy.settings.configure(lm=lm, rm=rm)

    # 4) Build RAG module
    rag = ResumeRAG(client)

    # 5) Tests
    tests = [
        # Resume-based
        "What are your strengths?",
        "Tell me about your recent project experience.",
        "Which BI tools and databases have you used?",
        # Small talk
        "Hi",
        "How are you?",
        # Inappropriate
        "You are an idiot",
    ]

    print("\n=== Test Runs ===")
    for q in tests:
        res = rag.forward(q)
        print(f"\nQ: {q}\nA: {res.final_answer}")

    # 6) Allow user input during runtime
    print("\n=== Interactive Q&A ===")
    while True:
        user_input = input("\nAsk a question about the resume (or type 'exit' to quit): ")

        if user_input.lower() == "exit":
            print("Exiting... Goodbye!")
            break

        # Process the user's question and get the response
        res = rag.forward(user_input)
        print(f"\nQ: {user_input}\nA: {res.final_answer}")

if __name__ == "__main__":
    main()

