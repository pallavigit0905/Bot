{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install qdrant-client[fastembed] dspy-ai dspy-qdrant python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVcmj-mmxza2",
        "outputId": "a9b14d09-5fef-40a8-d52f-9ddddd861047"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy-ai\n",
            "  Downloading dspy_ai-3.0.2-py3-none-any.whl.metadata (285 bytes)\n",
            "Collecting dspy-qdrant\n",
            "  Downloading dspy_qdrant-0.1.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting qdrant-client[fastembed]\n",
            "  Downloading qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting fastembed<0.8,>=0.7 (from qdrant-client[fastembed])\n",
            "  Downloading fastembed-0.7.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client[fastembed]) (1.74.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client[fastembed]) (2.0.2)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client[fastembed])\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client[fastembed]) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client[fastembed]) (2.11.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client[fastembed]) (2.5.0)\n",
            "Collecting dspy>=3.0.2 (from dspy-ai)\n",
            "  Downloading dspy-3.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting backoff>=2.2 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (1.5.1)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (1.101.0)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (2024.11.6)\n",
            "Collecting ujson>=5.8.0 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (2.32.4)\n",
            "Collecting optuna>=3.4.0 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting magicattr>=0.1.6 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting litellm>=1.64.0 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading litellm-1.76.0-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.0 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting json-repair>=0.30.0 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading json_repair-0.50.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (8.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (4.10.0)\n",
            "Collecting asyncer==0.0.8 (from dspy>=3.0.2->dspy-ai)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (3.1.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (13.9.4)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.2->dspy-ai) (3.5.0)\n",
            "Collecting gepa==0.0.4 (from gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai)\n",
            "  Downloading gepa-0.0.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (4.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.12/dist-packages (from fastembed<0.8,>=0.7->qdrant-client[fastembed]) (0.34.4)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed<0.8,>=0.7->qdrant-client[fastembed])\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mmh3<6.0.0,>=4.1.0 (from fastembed<0.8,>=0.7->qdrant-client[fastembed])\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed<0.8,>=0.7->qdrant-client[fastembed])\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from fastembed<0.8,>=0.7->qdrant-client[fastembed]) (11.3.0)\n",
            "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed<0.8,>=0.7->qdrant-client[fastembed])\n",
            "  Downloading py_rust_stemmers-0.1.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from fastembed<0.8,>=0.7->qdrant-client[fastembed]) (0.21.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy>=3.0.2->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (1.1.8)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (3.12.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (8.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (4.25.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (0.11.0)\n",
            "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed<0.8,>=0.7->qdrant-client[fastembed])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (1.13.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.2->dspy-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.2->dspy-ai) (0.10.0)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.4.0->dspy>=3.0.2->dspy-ai)\n",
            "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna>=3.4.0->dspy>=3.0.2->dspy-ai)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.2->dspy-ai) (2.0.43)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy>=3.0.2->dspy-ai) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.2->dspy-ai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.2->dspy-ai) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (1.20.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.2->dspy-ai) (1.1.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (2.2.2)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (0.70.16)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.2->dspy-ai) (0.27.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=3.0.2->dspy-ai) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=3.0.2->dspy-ai) (3.2.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed<0.8,>=0.7->qdrant-client[fastembed])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed<0.8,>=0.7->qdrant-client[fastembed]) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.6->gepa==0.0.4->gepa[dspy]==0.0.4->dspy>=3.0.2->dspy-ai) (1.17.0)\n",
            "Downloading dspy_ai-3.0.2-py3-none-any.whl (1.1 kB)\n",
            "Downloading dspy_qdrant-0.1.3-py3-none-any.whl (4.7 kB)\n",
            "Downloading dspy-3.0.2-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading gepa-0.0.4-py3-none-any.whl (35 kB)\n",
            "Downloading fastembed-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.50.0-py3-none-any.whl (25 kB)\n",
            "Downloading litellm-1.76.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_rust_stemmers-0.1.5-cp312-cp312-manylinux_2_28_x86_64.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py-rust-stemmers, magicattr, ujson, portalocker, mmh3, loguru, json-repair, humanfriendly, diskcache, colorlog, backoff, coloredlogs, asyncer, alembic, optuna, onnxruntime, qdrant-client, litellm, fastembed, gepa, dspy, dspy-qdrant, dspy-ai\n",
            "Successfully installed alembic-1.16.5 asyncer-0.0.8 backoff-2.2.1 coloredlogs-15.0.1 colorlog-6.9.0 diskcache-5.6.3 dspy-3.0.2 dspy-ai-3.0.2 dspy-qdrant-0.1.3 fastembed-0.7.3 gepa-0.0.4 humanfriendly-10.0 json-repair-0.50.0 litellm-1.76.0 loguru-0.7.3 magicattr-0.1.6 mmh3-5.2.0 onnxruntime-1.22.1 optuna-4.5.0 portalocker-3.2.0 py-rust-stemmers-0.1.5 qdrant-client-1.15.1 ujson-5.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkuo6FAuxbBx",
        "outputId": "33c45c01-06dd-4b35-d410-e63e610550eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dummy fallback_resume.txt\n",
            "Warning: Resume text file not found at 'C:\\Users\\npall\\Desktop\\sample_resume.txt'. Using fallback path.\n",
            "Loaded resume: 6584 chars, split into 5 chunks.\n",
            "Indexed 5 chunks into Qdrant collection 'resume_chat_bot'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:03:52 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Runs ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:03:54 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are your strengths?\n",
            "A: My strengths include over 8 years of experience in data analytics and engineering, specializing in transforming legacy reporting into automated dashboards using tools like Tableau, Power BI, and Cognos. I excel in optimizing reporting processes, having reduced cycle times by 30% and improved report generation accuracy significantly. My technical skills in SQL and Python, combined with my ability to mentor teams and enhance decision-making efficiency, allow me to align business goals with data strategies effectively.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:03:56 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: Tell me about your recent project experience.\n",
            "A: In my recent projects, I have focused on data engineering and analytics, leveraging tools like Tableau, Power BI, and SQL to enhance reporting and data visualization. One significant project involved optimizing data pipelines and ETL processes, which resulted in a 25% reduction in data processing times, improving decision-making timeliness. I also developed dynamic dashboards that replaced legacy Excel reports, reducing manual processes by 35% and ensuring accurate, timely data delivery. Additionally, I conducted advanced data analysis on large healthcare datasets, uncovering trends that led to operational improvements and cost reductions. My role often involved collaborating with cross-functional teams to align business goals with data strategies, ensuring that our solutions met evolving client needs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:03:57 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n",
            "2025/08/30 17:03:58 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n",
            "2025/08/30 17:03:58 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: Which BI tools and databases have you used?\n",
            "A: I have experience using several BI tools including Tableau (advanced), Power BI, SSRS, Cognos, and QlikView. In terms of databases, I have worked with Microsoft SQL Server, Oracle, MySQL, and others, utilizing languages such as T-SQL and PL/SQL for database management and queries.\n",
            "\n",
            "Q: Hi\n",
            "A: Hi! I’m your resume bot. Ask me about experience, projects, strengths, tools, or anything on the resume.\n",
            "\n",
            "Q: How are you?\n",
            "A: Hi! I’m your resume bot. Ask me about experience, projects, strengths, tools, or anything on the resume.\n",
            "\n",
            "Q: You are an idiot\n",
            "A: I won’t engage with abusive language. If you have a genuine question about my resume, I’m happy to help.\n",
            "\n",
            "=== Interactive Q&A ===\n",
            "\n",
            "Ask a question about the resume (or type 'exit' to quit): hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:04:04 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: hi\n",
            "A: Hi! I’m your resume bot. Ask me about experience, projects, strengths, tools, or anything on the resume.\n",
            "\n",
            "Ask a question about the resume (or type 'exit' to quit): which BI tools and databases have you used ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:04:35 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: which BI tools and databases have you used ?\n",
            "A: I have used several BI tools including Tableau (advanced), Power BI, SSRS, Cognos, and QlikView. In terms of databases, I have experience with Microsoft SQL Server, Oracle, MySQL, MongoDB, and others.\n",
            "\n",
            "Ask a question about the resume (or type 'exit' to quit): Tell me about your recent project experience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:04:59 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: Tell me about your recent project experience\n",
            "A: In my recent projects, I have focused on transforming legacy reporting systems into intelligent, automated dashboards using tools like Tableau and Power BI. For instance, I developed real-time Tableau dashboards that improved data storytelling and user-centric insights for key clients. I also managed data pipelines and optimized ETL processes, which reduced data processing times by 25%. Additionally, I revamped fragmented information systems into cohesive platforms, enhancing operational excellence and client care quality. My work has consistently aimed at aligning business goals with data strategy, ensuring accurate and timely reporting across various sectors.\n",
            "\n",
            "Ask a question about the resume (or type 'exit' to quit): you are an idiot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:05:13 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: you are an idiot\n",
            "A: I won’t engage with abusive language. If you have a genuine question about my resume, I’m happy to help.\n",
            "\n",
            "Ask a question about the resume (or type 'exit' to quit): What are your strengths\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/30 17:05:48 WARNING dspy.primitives.module: Calling module.forward(...) on ResumeRAG directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are your strengths\n",
            "A: My strengths include a strong proficiency in data automation and reporting processes, demonstrated by my ability to reduce report generation time by 40% and cycle time by 30%. I have extensive experience with BI tools such as Tableau, Power BI, and Cognos, which I have used to create dynamic dashboards that enhance decision-making. Additionally, I excel in mentoring teams and ensuring data integrity, which helps streamline workflows and improve reporting accuracy. My technical skills, combined with my ability to align data strategies with business objectives, enable me to deliver impactful insights and drive operational improvements.\n",
            "\n",
            "Ask a question about the resume (or type 'exit' to quit): exit\n",
            "Exiting... Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Resume Q&A Bot with DSPy + Qdrant\n",
        "Updated version with fixed resume path and optimized chunking\n",
        "\"\"\"\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from qdrant_client import QdrantClient, models\n",
        "from qdrant_client.models import Filter\n",
        "from dspy_qdrant import QdrantRM\n",
        "import dspy\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set these in your environment or directly here:\n",
        "os.environ[\"QDRANT_CLOUD_URL\"] = \"https://37636242-68f3-42eb-bf03-070af2870e21.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "os.environ[\"QDRANT_API_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.BR4DSofdNLhI645sQO4a_X-F6vAVkO_Quj_1V5ae63w\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"OPENAI_MODEL\"] = \"gpt-4o-mini\"\n",
        "# -------------------------\n",
        "# Configuration\n",
        "# -------------------------\n",
        "COLLECTION_NAME = os.environ.get(\"QDRANT_COLLECTION_NAME\", \"resume_chat_bot\")\n",
        "# Update this to the full path of your resume or set RESUME_PATH in .env\n",
        "\n",
        "RESUME_PATH = os.environ.get(\"RESUME_PATH\", r\"C:\\Users\\npall\\Desktop\\sample_resume.txt\")\n",
        "#RESUME_PATH = os.environ.get(\"RESUME_PATH\", \"sample_resume.txt\")\n",
        "\n",
        "QDRANT_URL = os.environ.get(\"QDRANT_CLOUD_URL\")\n",
        "QDRANT_API_KEY = os.environ.get(\"QDRANT_API_KEY\")\n",
        "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Create a dummy fallback_resume.txt file to prevent FileNotFoundError\n",
        "dummy_content = \"\"\"\n",
        "\n",
        "PALLAVI NAGOTHI\n",
        "San Jose, CA |   +15134603211| □ •  npallavi0905@gmail.com | LinkedIn | Tableau Public\n",
        "\n",
        "Strategic and solutions-driven Analytics Engineer with 8+ years of experience transforming legacy reporting into intelligent, automated dashboards using Tableau, Power BI, SSRS, and Cognos. Adept at aligning business goals with data strategy, translating complex requirements into compelling visual narratives, and streamlining reporting workflows across large-scale, cross-functional teams. Proven success in healthcare, finance, insurance, and aviation domains—delivering KPI-rich dashboards, accelerating decision-making, and ensuring compliance with HIPAA and industry standards. Skilled in SQL, Python, and Alteryx with a passion for mentoring teams and driving data integrity, visualization standardization, and reporting excellence.\n",
        "\n",
        "Tableau | Power BI Reporting | Cognos BERT Reports | SSRS Report Development | Alteryx | Data Visualization | AWS | Data Pipelining | SQL Query Optimization | Python Automation | KPI Analysis | Agile/Scrum Methodologies | Data Validation & QA | Stakeholder Communication | Dashboard Standardization and performance improvemnent\n",
        "\n",
        "\n",
        "Role: Recruited to automate legacy Excel-based reporting, transitioning to dynamic Tableau, SSRS, and BERT Cognos reports while standardizing dashboards, mentoring teams, validating KPIs, and managing ServiceNow work tickets.\n",
        "•\tReplaced legacy Excel reports with dynamic dashboards, reducing manual processes by 35% and improving reporting accuracy and consistency, ensuring flawless, time-critical month-end, quarter-end data delivery.\n",
        "•\tDeveloped and automated dynamic dashboards using Tableau, Power BI, SSRS, and BERT Cognos, replacing legacy Excel reports to enhance data accuracy and visualization and service ticket workflows for Boeing.\n",
        "•\tStandardized reporting frameworks and validated KPIs, ensuring consistency, reliability, and alignment with business goals across functions.\n",
        "•\tMentored junior analysts and offshore teams, enabling faster ramp-up on tools, processes, and data governance best practices.\n",
        "•\tManaged and resolved ServiceNow work tickets and ServiceNow's IT Operations Management software by ensuring data quality and addressing report-related issues and implementing enhancements to support evolving user requirements.\n",
        "\n",
        "Role: Selected as Business Data Analyst to bridge business goals with data strategy by translating complex requirements into actionable insights, enhancing decision-making through analytical storytelling, and streamlining reporting processes across cross- functional teams.\n",
        "•\tRevamped legacy Medicaid systems by crafting insightful Tableau and Power BI dashboards, accelerating claims analysis and ensuring seamless HIPAA-compliant data transitions with exceptional precision and clarity.\n",
        "•\tElevated reporting accuracy by enhancing complex Tableau and Power BI dashboards, standardizing visuals aligned with evolving client metrics.\n",
        "•\tConducted advanced data analysis of large healthcare datasets, uncovering trends and inefficiencies that contributed to operational improvements and cost reductions.\n",
        "•\tManaged data pipelines and optimized ETL processes, reducing data processing times by 25%, and enhancing the timeliness of decision-making.\n",
        "\n",
        "\n",
        "Role: Recruited to streamline data storytelling and user-centric insights by developing real-time Tableau dashboards, optimizing SQL performance, and aligning BI solutions with evolving business needs and risk gaps for key clients.\n",
        "•\tRevamped a fragmented Information System for Amex into a cohesive, user-adaptive platform—boosting client care quality and operational excellence through compelling Tableau visualizations.\n",
        "•\tCrafted high-impact Tableau dashboards showcasing cost trends of nationwide bank projects for Bank of America, enabling senior leadership to make swift, data-backed investment decisions.\n",
        "•\tSeamlessly migrated legacy Cognos dashboards to vibrant Tableau visuals, empowering executives with accurate travel forecasts and agile KPI-driven decision-making across global markets for AMEX.\n",
        "•\tFacilitated business process mapping and identified inefficiencies in data workflows, Led the optimization of reporting processes by implementing data automation and reducing cycle time by 30%.\n",
        "\n",
        "Role: Hired to transform academic and payroll insights into strategic assets by designing dynamic Cognos dashboards, resolving performance bottlenecks, and enabling mobile-friendly, offline reporting for data-driven institutional decisions for client-Rakesh Institution.\n",
        "•\tReduced report generation time by 40% through optimization of complex Cognos queries and implementation of concurrent query execution.\n",
        "•\tEnabled 100+ faculty and administrators to access real-time student performance data via mobile-optimized active reports, enhancing academic decision-making efficiency.\n",
        "\n",
        "•\tMasters in Software Engineering | Portland State University | 2016\n",
        "•\tBachelors in Information Technology | Jawaharlal Nehru Technological University | 2009\n",
        "\n",
        "•\tBI Tools: Power BI 1.14, Tableau 2022, 2019.2/10.5/10.4/9.02, SSRS (2012), Cognos BI suite (10.X), Alteryx\n",
        "•\tDatabase Languages: T-SQL, PL/SQL, MySQL\n",
        "•\tDatabases: Microsoft SQL Server 2016/2014/2012, NOSQL, Oracle 10 g/9i, DB2, MySQL, Amazon Simple DB, Mongo DB, Snowflake\n",
        "•\tCloud Platforms & Data Solutions: AWS (Amazon Redshift), Azure Data Factory, Cloud Data Integration\n",
        "•\tDatabase Technologies & Data Integration: SQL Server, Oracle, MySQL, Snowflake, MongoDB, Amazon Redshift, ETL Processes\n",
        "•\tAutomation & Data Engineering: Python (Data Analysis, Workflow Automation), SQL/PL-SQL (Complex Queries, Performance Tuning, Data Transformation)\n",
        "•\tData Visualization & BI Tools: Tableau (Advanced), Power BI, SSRS, Cognos, QlikView, Interactive Dashboards, Real- time Analytics, Data Storytelling\n",
        "•\tBusiness Analysis & Requirements Gathering: Stakeholder Engagement, Business Process Reengineering, Requirements Definition, Gap Analysis, User Story Mapping, Solution Design, Business Case Development.\n",
        "•\tAdvanced Data Analytics & Reporting: Predictive Analytics, Statistical Modelling, Data Mining, Root Cause Analysis, Trend Analysis, Financial & Operational Reporting\n",
        "•\tETL Tool: SSIS, Informatica\n",
        "•\tLanguages: Java, Python, HTML, XML, Android App development, UML\n",
        "•\tIDE’s/Tools: Visual Studio, SSDT, MS Excel, Excel Reports, Visio, SharePoint, MS Project, MS Word, Service Now ITSM\n",
        "•\tSoftware Methodologies: Waterfall, Agile – Scrum, RUP\n",
        "•\tProject Management & Agile Methodology: Agile, Scrum, Jira, Azure DevOps, MS Project, Cross-Functional Team Collaboration.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fallback_resume.txt\", \"w\") as f:\n",
        "    f.write(dummy_content)\n",
        "\n",
        "print(\"Created dummy fallback_resume.txt\")\n",
        "\n",
        "# -------------------------\n",
        "# Helpers: text loading + chunking\n",
        "# -------------------------\n",
        "\n",
        "def load_resume_text(path: str) -> str:\n",
        "    # Try to load the main file path\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Warning: Resume text file not found at '{path}'. Using fallback path.\")\n",
        "        # Specify an alternative fallback path here, making sure it's valid\n",
        "        fallback_path = \"fallback_resume.txt\"  # Make sure this file exists or specify another correct path\n",
        "        if not os.path.exists(fallback_path):\n",
        "            raise FileNotFoundError(f\"Resume text file not found at '{fallback_path}'. Please check the path.\")\n",
        "        path = fallback_path  # Update the path to fallback path\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def simple_chunk(text: str, max_tokens: int = 220, overlap: int = 40) -> List[str]:\n",
        "    \"\"\"Very simple word-based chunker approximating tokens with words.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    step = max(1, max_tokens - overlap)\n",
        "    for start in range(0, len(words), step):\n",
        "        end = min(len(words), start + max_tokens)\n",
        "        chunk = \" \".join(words[start:end]).strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        if end == len(words):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "# -------------------------\n",
        "# Qdrant setup + indexing\n",
        "# -------------------------\n",
        "def connect_qdrant() -> QdrantClient:\n",
        "    if not QDRANT_URL or not QDRANT_API_KEY:\n",
        "        raise RuntimeError(\"Missing Qdrant credentials. Set QDRANT_CLOUD_URL and QDRANT_API_KEY.\")\n",
        "    client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=QDRANT_API_KEY,\n",
        "        timeout=60.0,\n",
        "        prefer_grpc=True,\n",
        "    )\n",
        "    return client\n",
        "\n",
        "def ensure_collection(client: QdrantClient):\n",
        "    \"\"\"Create collection with dense + colbert multivectors.\"\"\"\n",
        "    exists = client.collection_exists(COLLECTION_NAME)\n",
        "    if not exists:\n",
        "        client.create_collection(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            vectors_config={\n",
        "                \"dense\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
        "                \"colbert\": models.VectorParams(\n",
        "                    size=128,\n",
        "                    distance=models.Distance.COSINE,\n",
        "                    multivector_config=models.MultiVectorConfig(\n",
        "                        comparator=models.MultiVectorComparator.MAX_SIM\n",
        "                    ),\n",
        "                    hnsw_config=models.HnswConfigDiff(m=0)\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "def embed_and_upsert_resume(client: QdrantClient, chunks: List[str]):\n",
        "    \"\"\"Encode chunks with FastEmbed dense + ColBERT and upsert as multivector points.\"\"\"\n",
        "    from fastembed import TextEmbedding, LateInteractionTextEmbedding\n",
        "\n",
        "    dense_model = TextEmbedding(\"BAAI/bge-small-en\")\n",
        "    colbert_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "    points = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        dense_doc = models.Document(text=chunk, model=\"BAAI/bge-small-en\")\n",
        "        colbert_doc = models.Document(text=chunk, model=\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "        points.append(\n",
        "            models.PointStruct(\n",
        "                id=i,\n",
        "                vector={\n",
        "                    \"dense\": dense_doc,\n",
        "                    \"colbert\": colbert_doc\n",
        "                },\n",
        "                payload={\n",
        "                    \"chunk_text\": chunk,\n",
        "                    \"category\": \"resume\",\n",
        "                    \"source\": \"user_resume\"\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Batch to avoid payload limits\n",
        "    BATCH = 8\n",
        "    for start in range(0, len(points), BATCH):\n",
        "        batch = points[start:start + BATCH]\n",
        "        client.upsert(collection_name=COLLECTION_NAME, points=batch)\n",
        "\n",
        "# -------------------------\n",
        "# DSPy Integration\n",
        "# -------------------------\n",
        "def init_lm() -> dspy.LM:\n",
        "    \"\"\"Initialize the LM for DSPy.\"\"\"\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Set OPENAI_API_KEY for language model access.\")\n",
        "    lm = dspy.LM(OPENAI_MODEL, max_tokens=512, api_key=api_key)\n",
        "    return lm\n",
        "\n",
        "def init_rm(client: QdrantClient) -> QdrantRM:\n",
        "    rm = QdrantRM(\n",
        "        qdrant_collection_name=COLLECTION_NAME,\n",
        "        qdrant_client=client,\n",
        "        vector_name=\"dense\",\n",
        "        document_field=\"chunk_text\",\n",
        "        k=8\n",
        "    )\n",
        "    return rm\n",
        "\n",
        "# -------------------------\n",
        "# ColBERT reranker using native multivector query\n",
        "# -------------------------\n",
        "def rerank_with_colbert(client: QdrantClient, query_text: str, limit: int = 5) -> List[str]:\n",
        "    from fastembed import TextEmbedding, LateInteractionTextEmbedding\n",
        "\n",
        "    dense_model = TextEmbedding(\"BAAI/bge-small-en\")\n",
        "    colbert_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "    dense_query = list(dense_model.embed(query_text))[0]\n",
        "    colbert_query = list(colbert_model.embed(query_text))[0]\n",
        "\n",
        "    results = client.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        prefetch=models.Prefetch(query=dense_query, using=\"dense\"),\n",
        "        query=colbert_query,\n",
        "        using=\"colbert\",\n",
        "        limit=limit,\n",
        "        with_payload=True,\n",
        "        query_filter=Filter(must=[]),\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    for p in results.points:\n",
        "        docs.append(p.payload.get(\"chunk_text\", \"\"))\n",
        "    return docs\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Guardrails: classify inputs\n",
        "# -------------------------\n",
        "ABUSE_LIST = {\"idiot\", \"stupid\", \"dumb\", \"shut up\", \"moron\", \"trash\", \"bastard\", \"jerk\"}\n",
        "\n",
        "def is_abusive(text: str) -> bool:\n",
        "    t = text.lower()\n",
        "    return any(w in t for w in ABUSE_LIST)\n",
        "\n",
        "SMALL_TALK_PATTERNS = [\n",
        "    r\"^\\s*hi\\s*$\", r\"^\\s*hello\\s*$\", r\"^\\s*hey\\s*$\",\n",
        "    r\"how are you\\??\", r\"what'?s up\\??\", r\"good (morning|afternoon|evening)\"\n",
        "]\n",
        "\n",
        "def is_small_talk(text: str) -> bool:\n",
        "    t = text.strip().lower()\n",
        "    for pat in SMALL_TALK_PATTERNS:\n",
        "        if re.search(pat, t):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# -------------------------\n",
        "# DSPy Signature & Module\n",
        "# -------------------------\n",
        "class ResumeAnswer(dspy.Signature):\n",
        "    question = dspy.InputField(desc=\"User question about the resume\")\n",
        "    context = dspy.InputField(desc=\"Relevant resume text chunks\")\n",
        "    final_answer = dspy.OutputField(desc=\"Helpful answer grounded in the resume, or a polite response\")\n",
        "\n",
        "class ResumeRAG(dspy.Module):\n",
        "    def __init__(self, client: QdrantClient):\n",
        "        super().__init__()\n",
        "        self.client = client\n",
        "\n",
        "    def forward(self, question: str):\n",
        "        # 1) Guardrails\n",
        "        if is_abusive(question):\n",
        "            class Dummy:\n",
        "                final_answer = \"I won’t engage with abusive language. If you have a genuine question about my resume, I’m happy to help.\"\n",
        "            return Dummy()\n",
        "\n",
        "        if is_small_talk(question):\n",
        "            class Dummy:\n",
        "                final_answer = \"Hi! I’m your resume bot. Ask me about experience, projects, strengths, tools, or anything on the resume.\"\n",
        "            return Dummy()\n",
        "\n",
        "        # 2) Retrieve + rerank\n",
        "        reranked = rerank_with_colbert(self.client, question, limit=6)\n",
        "        context = \"\\n\".join(reranked)\n",
        "\n",
        "        # 3) Compose a grounded answer with DSPy\n",
        "        prompt = dspy.ChainOfThought(ResumeAnswer)\n",
        "        return prompt(question=question, context=context)\n",
        "\n",
        "# -------------------------\n",
        "# Main (index + test)\n",
        "# -------------------------\n",
        "\n",
        "def main():\n",
        "    # 1) Load + chunk resume\n",
        "    resume_text = load_resume_text(RESUME_PATH)\n",
        "    chunks = simple_chunk(resume_text, max_tokens=220, overlap=40)\n",
        "    print(f\"Loaded resume: {len(resume_text)} chars, split into {len(chunks)} chunks.\")\n",
        "\n",
        "    # 2) Connect + ensure collection + upsert (idempotent upserts are fine for demo)\n",
        "    client = connect_qdrant()\n",
        "    ensure_collection(client)\n",
        "    embed_and_upsert_resume(client, chunks)\n",
        "    print(f\"Indexed {len(chunks)} chunks into Qdrant collection '{COLLECTION_NAME}'.\")\n",
        "\n",
        "    # 3) Init LM + RM for DSPy\n",
        "    lm = init_lm()\n",
        "    rm = init_rm(client)\n",
        "    dspy.settings.configure(lm=lm, rm=rm)\n",
        "\n",
        "    # 4) Build RAG module\n",
        "    rag = ResumeRAG(client)\n",
        "\n",
        "    # 5) Tests\n",
        "    tests = [\n",
        "        # Resume-based\n",
        "        \"What are your strengths?\",\n",
        "        \"Tell me about your recent project experience.\",\n",
        "        \"Which BI tools and databases have you used?\",\n",
        "        # Small talk\n",
        "        \"Hi\",\n",
        "        \"How are you?\",\n",
        "        # Inappropriate\n",
        "        \"You are an idiot\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== Test Runs ===\")\n",
        "    for q in tests:\n",
        "        res = rag.forward(q)\n",
        "        print(f\"\\nQ: {q}\\nA: {res.final_answer}\")\n",
        "\n",
        "    # 6) Allow user input during runtime\n",
        "    print(\"\\n=== Interactive Q&A ===\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nAsk a question about the resume (or type 'exit' to quit): \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Exiting... Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Process the user's question and get the response\n",
        "        res = rag.forward(user_input)\n",
        "        print(f\"\\nQ: {user_input}\\nA: {res.final_answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09329836",
        "outputId": "cd978984-e63d-4dc2-fbbe-7819388042b6"
      },
      "source": [],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dummy fallback_resume.txt\n"
          ]
        }
      ]
    }
  ]
}